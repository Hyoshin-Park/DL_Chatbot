{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgWvKw7BU1zI",
        "outputId": "d5ba0b21-25df-4330-f939-be041f92ddb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (0.20.0)\n",
            "Requirement already satisfied: pandas>=1.2.3 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from openai) (1.4.3)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from openai) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.20 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from openai) (2.28.1)\n",
            "Requirement already satisfied: pandas-stubs>=1.1.0.11 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from openai) (1.4.3.220710)\n",
            "Requirement already satisfied: tqdm in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from openai) (4.64.0)\n",
            "Requirement already satisfied: et-xmlfile in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from pandas>=1.2.3->openai) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from pandas>=1.2.3->openai) (1.22.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from requests>=2.20->openai) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from requests>=2.20->openai) (2022.6.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from requests>=2.20->openai) (3.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from tqdm->openai) (0.4.5)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.3->openai) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dkMwO3JNKx4",
        "outputId": "440bf997-ea9b-4062-a4f6-a28c8c466005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"length\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \" line\\\"></p> </\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1658150354,\n",
            "  \"id\": \"cmpl-5VKwUhvr6vXSNfFTQKOh5953JIGaj\",\n",
            "  \"model\": \"davinci\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 5,\n",
            "    \"prompt_tokens\": 4,\n",
            "    \"total_tokens\": 9\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"      # API 키 입력\n",
        "response = openai.Completion.create(engine=\"davinci\", prompt=\"This is a test\", max_tokens=5)\n",
        "print(response)     # 응답 확인\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukCofTWU3nA",
        "outputId": "a53e076e-b055-43c9-fa8c-9ee45aeb3dca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt3-sandbox'...\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/shreyashankar/gpt3-sandbox.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16ND_EqtEhPo",
        "outputId": "b1156ee4-140a-473e-c50c-7e9979c917c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nihso\\Documents\\ds_study\\source_code\\Deeplearning\\gpt3-sandbox\n"
          ]
        }
      ],
      "source": [
        "cd gpt3-sandbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVB7q5V4EF9Q",
        "outputId": "da4e1cc6-779f-4efe-b2d5-3d68ba3e566a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting astroid==2.4.2\n",
            "  Downloading astroid-2.4.2-py3-none-any.whl (213 kB)\n",
            "     ------------------------------------- 214.0/214.0 kB 12.7 MB/s eta 0:00:00\n",
            "Collecting certifi==2020.6.20\n",
            "  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
            "     -------------------------------------- 156.6/156.6 kB 9.8 MB/s eta 0:00:00\n",
            "Collecting chardet==3.0.4\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "     ---------------------------------------- 133.4/133.4 kB ? eta 0:00:00\n",
            "Collecting click==7.1.2\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "     ---------------------------------------- 82.8/82.8 kB ? eta 0:00:00\n",
            "Collecting Flask==1.1.2\n",
            "  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\n",
            "     ---------------------------------------- 94.6/94.6 kB ? eta 0:00:00\n",
            "Collecting idna==2.10\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "     ---------------------------------------- 58.8/58.8 kB 3.2 MB/s eta 0:00:00\n",
            "Collecting itsdangerous==1.1.0\n",
            "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting Jinja2==2.11.3\n",
            "  Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\n",
            "     ---------------------------------------- 125.7/125.7 kB ? eta 0:00:00\n",
            "Collecting MarkupSafe==1.1.1\n",
            "  Downloading MarkupSafe-1.1.1-cp38-cp38-win_amd64.whl (16 kB)\n",
            "Collecting openai==0.2.4\n",
            "  Downloading openai-0.2.4.tar.gz (157 kB)\n",
            "     ---------------------------------------- 157.1/157.1 kB ? eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting pylint==2.5.3\n",
            "  Downloading pylint-2.5.3-py3-none-any.whl (324 kB)\n",
            "     ------------------------------------- 324.5/324.5 kB 19.6 MB/s eta 0:00:00\n",
            "Collecting python-dotenv==0.14.0\n",
            "  Downloading python_dotenv-0.14.0-py2.py3-none-any.whl (17 kB)\n",
            "Collecting requests==2.27.1\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "     ---------------------------------------- 63.1/63.1 kB ? eta 0:00:00\n",
            "Collecting six==1.15.0\n",
            "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting urllib3==1.26.5\n",
            "  Downloading urllib3-1.26.5-py2.py3-none-any.whl (138 kB)\n",
            "     ---------------------------------------- 138.1/138.1 kB ? eta 0:00:00\n",
            "Collecting Werkzeug==1.0.1\n",
            "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
            "     ---------------------------------------- 298.6/298.6 kB ? eta 0:00:00\n",
            "Requirement already satisfied: wrapt~=1.11 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from astroid==2.4.2->-r api/requirements.txt (line 1)) (1.14.1)\n",
            "Collecting lazy-object-proxy==1.4.*\n",
            "  Downloading lazy_object_proxy-1.4.3-cp38-cp38-win_amd64.whl (21 kB)\n",
            "Collecting mccabe<0.7,>=0.6\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from pylint==2.5.3->-r api/requirements.txt (line 11)) (0.4.5)\n",
            "Requirement already satisfied: toml>=0.7.1 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from pylint==2.5.3->-r api/requirements.txt (line 11)) (0.10.2)\n",
            "Collecting isort<5,>=4.2.5\n",
            "  Downloading isort-4.3.21-py2.py3-none-any.whl (42 kB)\n",
            "     ---------------------------------------- 42.3/42.3 kB ? eta 0:00:00\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from requests==2.27.1->-r api/requirements.txt (line 13)) (2.0.4)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (setup.py): started\n",
            "  Building wheel for openai (setup.py): finished with status 'done'\n",
            "  Created wheel for openai: filename=openai-0.2.4-py3-none-any.whl size=170716 sha256=49f3cad1942f98ac69baf484d3bef0f0cd1b811c32427458dc00b6550d3584ed\n",
            "  Stored in directory: c:\\users\\nihso\\appdata\\local\\pip\\cache\\wheels\\b7\\f3\\1c\\f3735f96602735b985645ae9f9c15040f11aee8165b84b6f00\n",
            "Successfully built openai\n",
            "Installing collected packages: python-dotenv, mccabe, chardet, certifi, Werkzeug, urllib3, six, MarkupSafe, lazy-object-proxy, itsdangerous, isort, idna, click, requests, Jinja2, astroid, pylint, openai, Flask\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.6.15\n",
            "    Uninstalling certifi-2022.6.15:\n",
            "      Successfully uninstalled certifi-2022.6.15\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 2.1.2\n",
            "    Uninstalling Werkzeug-2.1.2:\n",
            "      Successfully uninstalled Werkzeug-2.1.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.9\n",
            "    Uninstalling urllib3-1.26.9:\n",
            "      Successfully uninstalled urllib3-1.26.9\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.1\n",
            "    Uninstalling MarkupSafe-2.1.1:\n",
            "      Successfully uninstalled MarkupSafe-2.1.1\n",
            "  Attempting uninstall: itsdangerous\n",
            "    Found existing installation: itsdangerous 2.1.2\n",
            "    Uninstalling itsdangerous-2.1.2:\n",
            "      Successfully uninstalled itsdangerous-2.1.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.3\n",
            "    Uninstalling idna-3.3:\n",
            "      Successfully uninstalled idna-3.3\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.0.4\n",
            "    Uninstalling click-8.0.4:\n",
            "      Successfully uninstalled click-8.0.4\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.28.1\n",
            "    Uninstalling requests-2.28.1:\n",
            "      Successfully uninstalled requests-2.28.1\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 3.0.3\n",
            "    Uninstalling Jinja2-3.0.3:\n",
            "      Successfully uninstalled Jinja2-3.0.3\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.20.0\n",
            "    Uninstalling openai-0.20.0:\n",
            "      Successfully uninstalled openai-0.20.0\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 2.1.2\n",
            "    Uninstalling Flask-2.1.2:\n",
            "      Successfully uninstalled Flask-2.1.2\n",
            "Successfully installed Flask-1.1.2 Jinja2-2.11.3 MarkupSafe-1.1.1 Werkzeug-1.0.1 astroid-2.4.2 certifi-2020.6.20 chardet-3.0.4 click-7.1.2 idna-2.10 isort-4.3.21 itsdangerous-1.1.0 lazy-object-proxy-1.4.3 mccabe-0.6.1 openai-0.2.4 pylint-2.5.3 python-dotenv-0.14.0 requests-2.27.1 six-1.15.0 urllib3-1.26.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fbprophet 0.7.1 requires cmdstanpy==0.9.5, which is not installed.\n",
            "fbprophet 0.7.1 requires setuptools-git>=1.2, which is not installed.\n",
            "sentence-transformers 2.2.0 requires transformers<5.0.0,>=4.6.0, but you have transformers 4.5.1 which is incompatible.\n",
            "arviz 0.11.2 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.2.0 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "!pip install -r api/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMC0dRd6EKKY",
        "outputId": "2a7795c1-8e45-4dc9-f220-4a2a2e8a817f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject list at 0x1d64a716db0> JSON: {\n",
              "  \"data\": [],\n",
              "  \"object\": \"list\"\n",
              "}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from api import GPT, Example, set_openai_key\n",
        "\n",
        "openai.FineTune.list() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "ji1DMW07FC4Q",
        "outputId": "66806551-9b41-416e-f246-cb9537814db0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unexpected exception formatting exception. Falling back to standard exception\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"C:\\Users\\nihso\\AppData\\Local\\Temp\\ipykernel_19268\\10662373.py\", line 1, in <cell line: 1>\n",
            "    run = openai.Completion.create(\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\openai\\api_resources\\completion.py\", line 31, in create\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 100, in create\n",
            "    organization,\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\openai\\api_requestor.py\", line 122, in request\n",
            "    str = info[\"name\"]\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\openai\\api_requestor.py\", line 329, in _interpret_response\n",
            "    rbody, rcode, rheaders, stream = self._client.request_with_retries(\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\openai\\api_requestor.py\", line 362, in _interpret_response_line\n",
            "    resp = OpenAIResponse(rbody, rcode, rheaders)\n",
            "openai.error.InvalidRequestError: That model does not exist\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1993, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
            "    frames.append(self.format_record(r))\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
            "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
            "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
            "    pieces = self.included_pieces\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
            "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
            "    pos = scope_pieces.index(self.executing_piece)\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
            "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
            "    return only(\n",
            "  File \"c:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
            "    raise NotOneValueFound('Expected one value, found 0')\n",
            "executing.executing.NotOneValueFound: Expected one value, found 0\n"
          ]
        }
      ],
      "source": [
        "run = openai.Completion.create(\n",
        "  model=\"curie:ft-user-qnr4sk9wekrfyxjtw6jb2hwl-2021-08-24-13-46-43\",      # 모델명 입력\n",
        "  prompt=\"나는 너무 우울해\",     # 원하는 텍스트 입력\n",
        "  temperature=0.5,      # 디폴트 1. 0에 가까울수록 정제된 문장, 높아질수록 창의적 문장\n",
        "  max_tokens=50,       # 최대 2048토큰까지\n",
        "  n = 5                 # 결과물 개수 (토큰 사용량과 연관이 큼)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdE7S1JvO3zI"
      },
      "source": [
        "# kogpt3\n",
        "참고: https://www.dinolabs.ai/399"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBKK6NZbYOrx",
        "outputId": "deae7b1c-7dd7-4f3d-b03b-d5b3299cc552"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [WinError 5] 액세스가 거부되었습니다: 'C:\\\\Users\\\\nihso\\\\miniconda3\\\\envs\\\\ds_study\\\\Lib\\\\site-packages\\\\~orch\\\\lib\\\\asmjit.dll'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 모델을 불러오기 위한 필수 라이브러리를 설치합니다.\n",
        "!pip install -q torch~=1.9.0\n",
        "!pip install -q transformers~=4.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2BcRMWG-YOmV"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Can't load config for 'kakaobrain/kogpt'. Make sure that:\n\n- 'kakaobrain/kogpt' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'kakaobrain/kogpt' is the correct path to a directory containing a config.json file\n\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\transformers\\configuration_utils.py:242\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 242\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m\n\u001b[0;32m    243\u001b[0m config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_dict_from_json_file(resolved_config_file)\n",
            "\u001b[1;31mOSError\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\nihso\\Documents\\ds_study\\source_code\\Deeplearning\\Kobert.ipynb 셀 10\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000009?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000009?line=2'>3</a>\u001b[0m \u001b[39m# 텍스트를 토큰(모델에서 사용되는 텍스트 단위)으로 쪼개어주는 토크나이저를 불러옵니다.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000009?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000009?line=4'>5</a>\u001b[0m   \u001b[39m'\u001b[39;49m\u001b[39mkakaobrain/kogpt\u001b[39;49m\u001b[39m'\u001b[39;49m, revision\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mKoGPT6B-ryan1.5b-float16\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000009?line=5'>6</a>\u001b[0m   bos_token\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m[BOS]\u001b[39;49m\u001b[39m'\u001b[39;49m, eos_token\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m[EOS]\u001b[39;49m\u001b[39m'\u001b[39;49m, unk_token\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m[UNK]\u001b[39;49m\u001b[39m'\u001b[39;49m, pad_token\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m[PAD]\u001b[39;49m\u001b[39m'\u001b[39;49m, mask_token\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m[MASK]\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000009?line=6'>7</a>\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\transformers\\tokenization_auto.py:206\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m config \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    205\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m--> 206\u001b[0m     config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    208\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mbert-base-japanese\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(pretrained_model_name_or_path):\n\u001b[0;32m    209\u001b[0m     \u001b[39mreturn\u001b[39;00m BertJapaneseTokenizer\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\transformers\\configuration_auto.py:203\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    134\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\" Instantiates one of the configuration classes of the library\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m    from a pre-trained model configuration.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     config_dict, _ \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    205\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[0;32m    206\u001b[0m         config_class \u001b[39m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m]]\n",
            "File \u001b[1;32mc:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\transformers\\configuration_utils.py:251\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    246\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    247\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load config for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Make sure that:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m- \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is a correct model identifier listed on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m- or \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory containing a \u001b[39m\u001b[39m{\u001b[39;00mCONFIG_NAME\u001b[39m}\u001b[39;00m\u001b[39m file\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m     )\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(msg)\n\u001b[0;32m    253\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError:\n\u001b[0;32m    254\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    255\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt reach server at \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m to download configuration file or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mconfiguration file is not a valid JSON file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check network or file content here: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(config_file, resolved_config_file)\n\u001b[0;32m    258\u001b[0m     )\n",
            "\u001b[1;31mOSError\u001b[0m: Can't load config for 'kakaobrain/kogpt'. Make sure that:\n\n- 'kakaobrain/kogpt' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'kakaobrain/kogpt' is the correct path to a directory containing a config.json file\n\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# 텍스트를 토큰(모델에서 사용되는 텍스트 단위)으로 쪼개어주는 토크나이저를 불러옵니다.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',\n",
        "  bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "ZB-ek7yWcB90",
        "outputId": "8ed97d4e-d089-4b75-ef4a-c355861329e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current device: cpu\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c0a1ac4e59d42288b18bf841aadc0ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/839 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6f29f40cd024a42b2f3ef217956ea55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/11.5G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\nihso\\Documents\\ds_study\\source_code\\Deeplearning\\Kobert.ipynb 셀 11\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000010?line=2'>3</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000010?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCurrent device:\u001b[39m\u001b[39m'\u001b[39m, device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000010?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000010?line=5'>6</a>\u001b[0m   \u001b[39m'\u001b[39;49m\u001b[39mkakaobrain/kogpt\u001b[39;49m\u001b[39m'\u001b[39;49m, revision\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mKoGPT6B-ryan1.5b-float16\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000010?line=6'>7</a>\u001b[0m   pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000010?line=7'>8</a>\u001b[0m   torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16, low_cpu_mem_usage\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000010?line=8'>9</a>\u001b[0m )\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000010?line=9'>10</a>\u001b[0m _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval()\n",
            "File \u001b[1;32mc:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:419\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    418\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 419\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    420\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    421\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    422\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\transformers\\modeling_utils.py:1413\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1411\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1412\u001b[0m     \u001b[39mwith\u001b[39;00m no_init_weights(_enable\u001b[39m=\u001b[39m_fast_init):\n\u001b[1;32m-> 1413\u001b[0m         model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[0;32m   1415\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[0;32m   1416\u001b[0m     \u001b[39m# restore default dtype\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\transformers\\models\\gptj\\modeling_gptj.py:682\u001b[0m, in \u001b[0;36mGPTJForCausalLM.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[0;32m    681\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[1;32m--> 682\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m GPTJModel(config)\n\u001b[0;32m    683\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mn_embd, config\u001b[39m.\u001b[39mvocab_size)\n\u001b[0;32m    684\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_weights()\n",
            "File \u001b[1;32mc:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\transformers\\models\\gptj\\modeling_gptj.py:443\u001b[0m, in \u001b[0;36mGPTJModel.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mn_embd\n\u001b[0;32m    442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mvocab_size\n\u001b[1;32m--> 443\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwte \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mEmbedding(config\u001b[39m.\u001b[39;49mvocab_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim)\n\u001b[0;32m    444\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39membd_pdrop)\n\u001b[0;32m    445\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([GPTJBlock(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mn_layer)])\n",
            "File \u001b[1;32mc:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:140\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, device, dtype)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mif\u001b[39;00m _weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((num_embeddings, embedding_dim), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n\u001b[0;32m    141\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    142\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlist\u001b[39m(_weight\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m [num_embeddings, embedding_dim], \\\n\u001b[0;32m    143\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mShape of weight does not match num_embeddings and embedding_dim\u001b[39m\u001b[39m'\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:149\u001b[0m, in \u001b[0;36mEmbedding.reset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m     init\u001b[39m.\u001b[39;49mnormal_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[0;32m    150\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fill_padding_idx_with_zero()\n",
            "File \u001b[1;32mc:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\torch\\nn\\init.py:151\u001b[0m, in \u001b[0;36mnormal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormal_\u001b[39m(tensor: Tensor, mean: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m, std: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    139\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Fills the input Tensor with values drawn from the normal\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[39m    distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[39m        >>> nn.init.normal_(w)\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m     \u001b[39mreturn\u001b[39;00m _no_grad_normal_(tensor, mean, std)\n",
            "File \u001b[1;32mc:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\torch\\nn\\init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[0;32m     18\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 19\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mnormal_(mean, std)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# KoGPT model을 불러옵니다.\n",
        "# 기본 모델은 최소 32GB GPU RAM 을 필요로 하므로 더 작은 모델인 'KoGPT6B-ryan1.5b-float16'을 불러오겠습니다.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Current device:', device)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',\n",
        "  pad_token_id=tokenizer.eos_token_id,\n",
        "  torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
        ").to(device=device, non_blocking=True)\n",
        "_ = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oKjpC6ZcC5F"
      },
      "outputs": [],
      "source": [
        "# 입력 문장(prompt)을 받아 모델에서 생성된 결과를 보여주는 함수를 만듭니다.\n",
        "def gpt(prompt, max_length: int = 256):\n",
        "    with torch.no_grad():\n",
        "        # 입력문장을 토크나이저를 사용하여 토큰화\n",
        "        tokens = tokenizer.encode(prompt, return_tensors='pt').to(device=device, non_blocking=True)\n",
        "        # 토큰화된 문장을 입력으로 토큰형태의 새로운 문장 생성\n",
        "        gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=max_length)\n",
        "        # 생성된 문장을 다시 문자열 형태로 디코딩\n",
        "        generated = tokenizer.batch_decode(gen_tokens)[0]\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l3diJKZcD2N"
      },
      "outputs": [],
      "source": [
        "# 기존함수를 조금 수정하여 인터뷰를 위한 함수를 새로 생성하겠습니다.\n",
        "def gpt_interview(prompt, max_length: int = 256, ends_interview: bool = False):\n",
        "    with torch.no_grad():\n",
        "        # 입력문장을 토크나이저를 사용하여 토큰화\n",
        "        tokens = tokenizer.encode(prompt, return_tensors='pt').to(device=device, non_blocking=True)\n",
        "        # 토큰화된 문장을 입력으로 토큰형태의 새로운 문장 생성\n",
        "        gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=max_length)\n",
        "        # 생성된 문장을 다시 문자열 형태로 디코딩\n",
        "        generated = tokenizer.batch_decode(gen_tokens)[0]\n",
        "    generated_answer = generated[len(prompt)-3:]\n",
        "    if ends_interview:\n",
        "        end_idx = generated_answer.index('\\n', 2)\n",
        "        return generated[:len(prompt)+end_idx-3]\n",
        "    else:\n",
        "        end_idx = generated_answer.index('Q') \n",
        "        return generated[:len(prompt)+end_idx-3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYVgM3mObW_Q"
      },
      "source": [
        "# NEW KOGPT2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c-4yyp8cILD",
        "outputId": "2684165d-08cd-437b-b6a3-9e4d339d0f36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kobert-transformers==0.4.1 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (0.4.1)\n",
            "Requirement already satisfied: transformers>=2.9.1 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from kobert-transformers==0.4.1) (4.12.5)\n",
            "Requirement already satisfied: torch>=1.1.0 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from kobert-transformers==0.4.1) (1.9.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from torch>=1.1.0->kobert-transformers==0.4.1) (4.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers==0.4.1) (2022.3.15)\n",
            "Requirement already satisfied: filelock in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers==0.4.1) (3.7.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers==0.4.1) (4.64.0)\n",
            "Requirement already satisfied: sacremoses in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers==0.4.1) (0.0.53)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Using cached tokenizers-0.10.3-cp38-cp38-win_amd64.whl (2.0 MB)\n",
            "Requirement already satisfied: requests in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers==0.4.1) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers==0.4.1) (1.22.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers==0.4.1) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers==0.4.1) (0.7.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers==0.4.1) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from packaging>=20.0->transformers>=2.9.1->kobert-transformers==0.4.1) (3.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from tqdm>=4.27->transformers>=2.9.1->kobert-transformers==0.4.1) (0.4.5)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from requests->transformers>=2.9.1->kobert-transformers==0.4.1) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from requests->transformers>=2.9.1->kobert-transformers==0.4.1) (2020.6.20)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from requests->transformers>=2.9.1->kobert-transformers==0.4.1) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from requests->transformers>=2.9.1->kobert-transformers==0.4.1) (1.26.5)\n",
            "Requirement already satisfied: click in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from sacremoses->transformers>=2.9.1->kobert-transformers==0.4.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from sacremoses->transformers>=2.9.1->kobert-transformers==0.4.1) (1.1.0)\n",
            "Requirement already satisfied: six in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from sacremoses->transformers>=2.9.1->kobert-transformers==0.4.1) (1.15.0)\n",
            "Installing collected packages: tokenizers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.8.1rc1\n",
            "    Uninstalling tokenizers-0.8.1rc1:\n",
            "      Successfully uninstalled tokenizers-0.8.1rc1\n",
            "Successfully installed tokenizers-0.10.3\n",
            "Collecting transformers==3.0.2\n",
            "  Using cached transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "Requirement already satisfied: sacremoses in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers==3.0.2) (0.0.53)\n",
            "Requirement already satisfied: filelock in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers==3.0.2) (3.7.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers==3.0.2) (1.22.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers==3.0.2) (0.1.96)\n",
            "Requirement already satisfied: packaging in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers==3.0.2) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers==3.0.2) (2022.3.15)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "  Using cached tokenizers-0.8.1rc1-cp38-cp38-win_amd64.whl (1.9 MB)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers==3.0.2) (4.64.0)\n",
            "Requirement already satisfied: requests in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from transformers==3.0.2) (2.27.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from tqdm>=4.27->transformers==3.0.2) (0.4.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from packaging->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from requests->transformers==3.0.2) (2020.6.20)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from requests->transformers==3.0.2) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from requests->transformers==3.0.2) (1.26.5)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: six in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: click in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.10.3\n",
            "    Uninstalling tokenizers-0.10.3:\n",
            "      Successfully uninstalled tokenizers-0.10.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.12.5\n",
            "    Uninstalling transformers-4.12.5:\n",
            "      Successfully uninstalled transformers-4.12.5\n",
            "Successfully installed tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 2.2.0 requires transformers<5.0.0,>=4.6.0, but you have transformers 3.0.2 which is incompatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (1.9.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (from torch) (4.2.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1rc1 in c:\\users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages (0.8.1rc1)\n"
          ]
        }
      ],
      "source": [
        "!pip install kobert-transformers==0.4.1\n",
        "!pip install transformers==3.0.2\n",
        "!pip install torch\n",
        "!pip install tokenizers==0.8.1rc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FslACcWcm9fy"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "from transformers import BertConfig\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "#KoBERT\n",
        "kobert_config = {\n",
        "    'attention_probs_dropout_prob': 0.1,\n",
        "    'hidden_act': 'gelu',\n",
        "    'hidden_dropout_prob': 0.1,\n",
        "    'hidden_size': 768,\n",
        "    'initializer_range': 0.02,\n",
        "    'intermediate_size': 3072,\n",
        "    'max_position_embeddings': 512,\n",
        "    'num_attention_heads': 12,\n",
        "    'num_hidden_layers': 12,\n",
        "    'type_vocab_size': 2,\n",
        "    'vocab_size': 8002\n",
        "}\n",
        "\n",
        "def get_kobert_config():\n",
        "    return BertConfig.from_dict(kobert_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " C ����̺��� �������� �̸��� �����ϴ�.\n",
            " ���� �Ϸ� ��ȣ: C07C-E6FE\n",
            "\n",
            " c:\\Users\\nihso\\Documents\\ds_study\\source_code\\Deeplearning ���͸�\n",
            "\n",
            "2022-07-19  ���� 01:41    <DIR>          .\n",
            "2022-07-19  ���� 01:41    <DIR>          ..\n",
            "2022-06-20  ���� 04:09    <DIR>          .ipynb_checkpoints\n",
            "2022-07-16  ���� 07:30             7,915 1.2.10\n",
            "2022-07-16  ���� 09:38           889,842 ChatbotData.csv\n",
            "2022-07-16  ���� 07:46        47,078,520 chatbot_dataset_l.csv\n",
            "2022-07-16  ���� 07:47         3,251,876 chatbot_dataset_s.csv\n",
            "2022-06-20  ���� 07:08            90,994 chat_data.txt\n",
            "2022-05-25  ���� 07:58            24,587 CNN.ipynb\n",
            "2022-05-31  ���� 10:07    <DIR>          Data\n",
            "2022-05-25  ���� 05:21           221,576 deep learning.ipynb\n",
            "2022-05-30  ���� 05:55            90,763 DL Scratch.ipynb\n",
            "2022-07-12  ���� 08:10         2,112,388 DL.ipynb\n",
            "2022-06-21  ���� 08:04           853,577 DL_project.ipynb\n",
            "2022-06-20  ���� 01:57             1,519 DL_project.py\n",
            "2022-06-07  ���� 05:31        35,585,024 ethics.tar\n",
            "2022-06-13  ���� 05:42            90,768 GPT.ipynb\n",
            "2022-07-18  ���� 10:19    <DIR>          gpt3-sandbox\n",
            "2022-07-16  ���� 07:55            30,338 GPT3-test.ipynb\n",
            "2020-01-30  ���� 10:43            90,976 KETI_��ȭ������_�ϻ�_���ǽ�.txt\n",
            "2022-07-18  ���� 10:19            58,406 Kobert.ipynb\n",
            "2022-07-18  ���� 07:19    <DIR>          KoGPT2-chatbot\n",
            "2022-07-18  ���� 07:31            29,123 kogpt_test.ipynb\n",
            "2022-07-18  ���� 07:32    <DIR>          lightning_logs\n",
            "2022-07-14  ���� 10:31             1,432 main.py\n",
            "2022-05-31  ���� 02:14         1,011,880 maskman.ipynb\n",
            "2022-05-25  ���� 07:47           166,581 MNIST using DL.ipynb\n",
            "2022-07-07  ���� 11:53            17,797 NIKL ��ȭ ����.txt\n",
            "2022-07-07  ���� 04:27    <DIR>          NIKL_OM_2021_v1.0\n",
            "2022-06-07  ���� 03:07    <DIR>          test\n",
            "2022-06-22  ���� 02:03             2,062 tmp.txt\n",
            "2022-05-31  ���� 11:49            17,913 torch.ipynb\n",
            "2022-07-16  ���� 09:18             8,405 trainer_l.py\n",
            "2022-07-18  ���� 04:05             9,702 trainer_s.py\n",
            "2022-07-17  ���� 03:58             9,618 train_torch1.py\n",
            "2022-06-07  ���� 02:55             1,177 Untitled.ipynb\n",
            "2022-06-20  ���� 04:12            22,627 Untitled1.ipynb\n",
            "2022-06-21  ���� 05:10        10,010,513 wellness_dataset.csv\n",
            "2022-06-20  ���� 01:45           557,814 wellness_dataset_original.csv\n",
            "2022-07-18  ���� 10:49           104,603 wellness_dialog_answer.txt\n",
            "2022-07-18  ���� 10:50            10,331 wellness_dialog_category.txt\n",
            "2022-07-18  ���� 10:50           359,872 wellness_dialog_for_text_classification_all.txt\n",
            "2022-05-25  ���� 04:36             1,180 x09.txt\n",
            "2022-06-20  ���� 08:03        15,191,734 ������ȭ����ġ(����������)_Training.xlsx\n",
            "2022-06-29  ���� 08:31    <DIR>          ��������� ���� ���� ����ġ(���� 1.0)\n",
            "2022-06-20  ���� 04:55    <DIR>          �ѱ��� ��ȭ �����ͼ�\n",
            "              35�� ����         118,013,433 ����Ʈ\n",
            "              11�� ���͸�  76,553,990,144 ����Ʈ ����\n"
          ]
        }
      ],
      "source": [
        "!dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxJBoWp2nBOU",
        "outputId": "e9bbd398-5a5f-4687-a985-3214df335fb3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18c236f69c9c44499cb740241532ead4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/371k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c6c2726fa89442e9f13325d54c2b7fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/77.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<__main__.WellnessTextClassificationDataset object at 0x00000128299AFA60>\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from kobert_transformers import get_tokenizer\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class WellnessTextClassificationDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 file_path=\"./wellness_dialog_for_text_classification_all.txt\",\n",
        "                 num_label=359,\n",
        "                 device='cpu',\n",
        "                 max_seq_len=512,  # KoBERT max_length\n",
        "                 tokenizer=None\n",
        "                 ):\n",
        "        self.file_path = file_path\n",
        "        self.device = device\n",
        "        self.data = []\n",
        "        self.tokenizer = tokenizer if tokenizer is not None else get_tokenizer()\n",
        "\n",
        "        file = open(self.file_path, 'r', encoding='utf-8')\n",
        "\n",
        "        while True:\n",
        "            line = file.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            datas = line.split(\"    \")\n",
        "            index_of_words = self.tokenizer.encode(datas[0])\n",
        "            token_type_ids = [0] * len(index_of_words)\n",
        "            attention_mask = [1] * len(index_of_words)\n",
        "\n",
        "            # Padding Length\n",
        "            padding_length = max_seq_len - len(index_of_words)\n",
        "\n",
        "            # Zero Padding\n",
        "            index_of_words += [0] * padding_length\n",
        "            token_type_ids += [0] * padding_length\n",
        "            attention_mask += [0] * padding_length\n",
        "\n",
        "            # Label\n",
        "            label = int(datas[1][:-1])\n",
        "\n",
        "            data = {\n",
        "                'input_ids': torch.tensor(index_of_words).to(self.device),\n",
        "                'token_type_ids': torch.tensor(token_type_ids).to(self.device),\n",
        "                'attention_mask': torch.tensor(attention_mask).to(self.device),\n",
        "                'labels': torch.tensor(label).to(self.device)\n",
        "            }\n",
        "\n",
        "            self.data.append(data)\n",
        "\n",
        "        file.close()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data[index]\n",
        "        return item\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = WellnessTextClassificationDataset()\n",
        "    print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "siCPt2NCnMF4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from kobert_transformers import get_kobert_model\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "from transformers import BertPreTrainedModel\n",
        "\n",
        "class KoBERTforSequenceClassfication(BertPreTrainedModel):\n",
        "    def __init__(self,\n",
        "                 num_labels=359,\n",
        "                 hidden_size=768,\n",
        "                 hidden_dropout_prob=0.1,\n",
        "                 ):\n",
        "        super().__init__(get_kobert_config())\n",
        "\n",
        "        self.num_labels = num_labels\n",
        "        self.kobert = get_kobert_model()\n",
        "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids=None,\n",
        "            attention_mask=None,\n",
        "            token_type_ids=None,\n",
        "            position_ids=None,\n",
        "            head_mask=None,\n",
        "            inputs_embeds=None,\n",
        "            labels=None,\n",
        "    ):\n",
        "        outputs = self.kobert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "def kobert_input(tokenizer, str, device=None, max_seq_len=512):\n",
        "    index_of_words = tokenizer.encode(str)\n",
        "    token_type_ids = [0] * len(index_of_words)\n",
        "    attention_mask = [1] * len(index_of_words)\n",
        "\n",
        "    # Padding Length\n",
        "    padding_length = max_seq_len - len(index_of_words)\n",
        "\n",
        "    # Zero Padding\n",
        "    index_of_words += [0] * padding_length\n",
        "    token_type_ids += [0] * padding_length\n",
        "    attention_mask += [0] * padding_length\n",
        "\n",
        "    data = {\n",
        "        'input_ids': torch.tensor([index_of_words]).to(device),\n",
        "        'token_type_ids': torch.tensor([token_type_ids]).to(device),\n",
        "        'attention_mask': torch.tensor([attention_mask]).to(device),\n",
        "    }\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP3ubueTm1Mj",
        "outputId": "27e79ffa-8f13-4acd-b720-31970834e086"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "989330e2edb14477bd6c94afef0f90f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/426 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9b9cdb995694cd98163e4f2f7ffa8cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/369M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train(0):   0%|          | 2/1185 [01:36<15:46:34, 48.01s/it, Loss: 5.923 (5.847)]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\nihso\\Documents\\ds_study\\source_code\\Deeplearning\\Kobert.ipynb 셀 20\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000018?line=92'>93</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epoch):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000018?line=93'>94</a>\u001b[0m     epoch \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m offset\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000018?line=94'>95</a>\u001b[0m     loss \u001b[39m=\u001b[39m train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000018?line=95'>96</a>\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss)\n",
            "\u001b[1;32mc:\\Users\\nihso\\Documents\\ds_study\\source_code\\Deeplearning\\Kobert.ipynb 셀 20\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000018?line=23'>24</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000018?line=25'>26</a>\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000018?line=27'>28</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000018?line=28'>29</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000018?line=30'>31</a>\u001b[0m pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\torch\\_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    247\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    248\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    249\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    254\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 255\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
            "File \u001b[1;32mc:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\torch\\autograd\\__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> 147\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    148\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    149\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import dataloader\n",
        "from tqdm import tqdm\n",
        "from transformers import AdamW\n",
        "\n",
        "\n",
        "def train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step=0):\n",
        "    losses = []\n",
        "    train_start_index = train_step + 1 if train_step != 0 else 0\n",
        "    total_train_step = len(train_loader)\n",
        "    model.train()\n",
        "\n",
        "    with tqdm(total=total_train_step, desc=f\"Train({epoch})\") as pbar:\n",
        "        pbar.update(train_step)\n",
        "        for i, data in enumerate(train_loader, train_start_index):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**data)\n",
        "\n",
        "            loss = outputs[0]\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix_str(f\"Loss: {loss.item():.3f} ({np.mean(losses):.3f})\")\n",
        "\n",
        "            if i >= total_train_step or i % save_step == 0:\n",
        "                torch.save({\n",
        "                    'epoch': epoch,  # 현재 학습 epoch\n",
        "                    'model_state_dict': model.state_dict(),  # 모델 저장\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 저장\n",
        "                    'loss': loss.item(),  # Loss 저장\n",
        "                    'train_step': i,  # 현재 진행한 학습\n",
        "                    'total_train_step': len(train_loader)  # 현재 epoch에 학습 할 총 train step\n",
        "                }, save_ckpt_path)\n",
        "\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    root_path = \"./\"\n",
        "    data_path = f\"{root_path}wellness_dialog_for_text_classification_all.txt\"\n",
        "    checkpoint_path = f\"{root_path}\"\n",
        "    save_ckpt_path = f\"{checkpoint_path}/kobert-wellness-text-classification.pth\"\n",
        "\n",
        "    n_epoch = 1  # Num of Epoch\n",
        "    batch_size = 4   # 배치 사이즈 #Colab이 돌아가지 않아 4로 했으며, 증가시켜도 무방\n",
        "    ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    device = torch.device(ctx)\n",
        "    save_step = 100  # 학습 저장 주기\n",
        "    learning_rate = 5e-6  # Learning Rate\n",
        "\n",
        "    # WellnessTextClassificationDataset Data Loader\n",
        "    dataset = WellnessTextClassificationDataset(file_path=data_path, device=device)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = KoBERTforSequenceClassfication()\n",
        "    model.to(device)\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "\n",
        "    pre_epoch, pre_loss, train_step = 0, 0, 0\n",
        "    if os.path.isfile(save_ckpt_path):\n",
        "        checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "        pre_epoch = checkpoint['epoch']\n",
        "        train_step = checkpoint['train_step']\n",
        "        total_train_step = checkpoint['total_train_step']\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")\n",
        "\n",
        "    losses = []\n",
        "    offset = pre_epoch\n",
        "    for step in range(n_epoch):\n",
        "        epoch = step + offset\n",
        "        loss = train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step)\n",
        "        losses.append(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "RBWzW55Pm29w",
        "outputId": "4cc84569-86f3-44fc-eaf5-18fe9a18a710"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\nihso\\Documents\\ds_study\\source_code\\Deeplearning\\Kobert.ipynb 셀 21\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000019?line=65'>66</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(ctx)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000019?line=67'>68</a>\u001b[0m \u001b[39m# 저장한 Checkpoint 불러오기\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000019?line=68'>69</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(save_ckpt_path, map_location\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000019?line=70'>71</a>\u001b[0m model \u001b[39m=\u001b[39m KoBERTforSequenceClassfication()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nihso/Documents/ds_study/source_code/Deeplearning/Kobert.ipynb#ch0000019?line=71'>72</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39m\u001b[39mmodel_state_dict\u001b[39m\u001b[39m'\u001b[39m])\n",
            "File \u001b[1;32mc:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\torch\\serialization.py:600\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    596\u001b[0m     \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m     \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    598\u001b[0m     \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    599\u001b[0m     orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n\u001b[1;32m--> 600\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_reader(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    601\u001b[0m         \u001b[39mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[0;32m    602\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch.load\u001b[39m\u001b[39m'\u001b[39m\u001b[39m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    603\u001b[0m                           \u001b[39m\"\u001b[39m\u001b[39m dispatching to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch.jit.load\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (call \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch.jit.load\u001b[39m\u001b[39m'\u001b[39m\u001b[39m directly to\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    604\u001b[0m                           \u001b[39m\"\u001b[39m\u001b[39m silence this warning)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mUserWarning\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\nihso\\miniconda3\\envs\\ds_study\\lib\\site-packages\\torch\\serialization.py:242\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[1;34m(self, name_or_buffer)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name_or_buffer) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 242\u001b[0m     \u001b[39msuper\u001b[39m(_open_zipfile_reader, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileReader(name_or_buffer))\n",
            "\u001b[1;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "from kobert_transformers import get_tokenizer\n",
        "\n",
        "\n",
        "def load_wellness_answer():\n",
        "    root_path = \"./\"\n",
        "    category_path = f\"{root_path}wellness_dialog_category.txt\"\n",
        "    answer_path = f\"{root_path}wellness_dialog_answer.txt\"\n",
        "\n",
        "    c_f = open(category_path, 'r')\n",
        "    a_f = open(answer_path, 'r')\n",
        "\n",
        "    category_lines = c_f.readlines()\n",
        "    answer_lines = a_f.readlines()\n",
        "\n",
        "    category = {}\n",
        "    answer = {}\n",
        "    for line_num, line_data in enumerate(category_lines):\n",
        "        data = line_data.split('    ')\n",
        "        category[data[1][:-1]] = data[0]\n",
        "\n",
        "    for line_num, line_data in enumerate(answer_lines):\n",
        "        data = line_data.split('    ')\n",
        "        keys = answer.keys()\n",
        "        if (data[0] in keys):\n",
        "            answer[data[0]] += [data[1][:-1]]\n",
        "        else:\n",
        "            answer[data[0]] = [data[1][:-1]]\n",
        "\n",
        "    return category, answer\n",
        "\n",
        "\n",
        "def kobert_input(tokenizer, str, device=None, max_seq_len=512):\n",
        "    index_of_words = tokenizer.encode(str)\n",
        "    token_type_ids = [0] * len(index_of_words)\n",
        "    attention_mask = [1] * len(index_of_words)\n",
        "\n",
        "    # Padding Length\n",
        "    padding_length = max_seq_len - len(index_of_words)\n",
        "\n",
        "    # Zero Padding\n",
        "    index_of_words += [0] * padding_length\n",
        "    token_type_ids += [0] * padding_length\n",
        "    attention_mask += [0] * padding_length\n",
        "\n",
        "    data = {\n",
        "        'input_ids': torch.tensor([index_of_words]).to(device),\n",
        "        'token_type_ids': torch.tensor([token_type_ids]).to(device),\n",
        "        'attention_mask': torch.tensor([attention_mask]).to(device),\n",
        "    }\n",
        "    return data\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    root_path = \"./\"\n",
        "    checkpoint_path = f\"{root_path}\"\n",
        "    save_ckpt_path = f\"{checkpoint_path}/kobert-wellness-text-classification.pth\"\n",
        "\n",
        "    # 답변과 카테고리 불러오기\n",
        "    category, answer = load_wellness_answer()\n",
        "\n",
        "    ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    device = torch.device(ctx)\n",
        "\n",
        "    # 저장한 Checkpoint 불러오기\n",
        "    checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "\n",
        "    model = KoBERTforSequenceClassfication()\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    model.to(ctx)\n",
        "    model.eval()\n",
        "\n",
        "    tokenizer = get_tokenizer()\n",
        "\n",
        "    while 1:\n",
        "        sent = input('\\nQuestion: ')  # '요즘 기분이 우울한 느낌이에요'\n",
        "        data = kobert_input(tokenizer, sent, device, 512)\n",
        "\n",
        "        if '종료' in sent:\n",
        "            break\n",
        "\n",
        "        output = model(**data)\n",
        "\n",
        "        logit = output[0]\n",
        "        softmax_logit = torch.softmax(logit, dim=-1)\n",
        "        softmax_logit = softmax_logit.squeeze()\n",
        "\n",
        "        max_index = torch.argmax(softmax_logit).item()\n",
        "        max_index_value = softmax_logit[torch.argmax(softmax_logit)].item()\n",
        "\n",
        "        answer_list = answer[category[str(max_index)]]\n",
        "        answer_len = len(answer_list) - 1\n",
        "        answer_index = random.randint(0, answer_len)\n",
        "        print(f'Answer: {answer_list[answer_index]}, index: {max_index}, softmax_value: {max_index_value}')\n",
        "        print('-' * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIBPPV3Cvb6h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "새로운 채팅으로 챗봇 이용.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.12 ('ds_study')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "9d427dfaf44bfe573fbc934bba616458c60e55e884bd082a4375970f130de506"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
